{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキスト処理の機械学習におけるBi-Normal Separation(BNS)の効果について   \n",
    "## 背景  \n",
    "日本語テキストを機械学習で分類する機会があった。データは簡単に以下のような感じ。\n",
    "<p>\n",
    "<!-- <table border=\"1\"> -->\n",
    "<table border=\"3\">\n",
    "<tr>\n",
    "<td>入力データ</td> <td align=\"center\">ある観点</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>日本語テキスト1</td> <td align=\"center\">O</td> \n",
    "</tr>\n",
    "<tr>\n",
    "<td>日本語テキスト2</td> <td align=\"center\">X</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>日本語テキスト3</td> <td align=\"center\">O</td> \n",
    "</tr>\n",
    "</table>\n",
    "</p>  \n",
    "- 入力データの入力とある観点のO/X付けは別の人が行っている\n",
    "- \"ある観点\"は実際には複数あり、マルチラベルになっているがプロトタイプだったのでシングルラベルで処理   \n",
    "- データ数は少ない(３桁後半) \n",
    "- データサイズは日本語文字数で500から800\n",
    "- ポジティブデータの割合は小さい。観点によるがひと桁%〜30%  \n",
    "\n",
    "当初、言語処理100本ノックで作ったプログラム(Cabocha,Python,scikit-learn。第５章、第６章、第８章)を修正して処理を行ったが精度が上がらなかった。そこでフィーチャー(素性)の選択について調べている中でBi-Normal Separation(BNS)を知り適用してみたところ顕著な改善が見られた。  \n",
    "そのままはアップロードできないので言語処理100本ノックの第８章をベースにBNSの効果を見てみることにした。\n",
    "\n",
    "## Bi-Normal Separation(BNS)について  \n",
    "BNSについては以下の[レポート][1]を参考にした。  \n",
    "> BNS Feature Scaling: An Improved Representation over TF·IDF for SVM\n",
    "> Text Classification\n",
    "> George Forman\n",
    "> HP Laboratories\n",
    "> HPL-2007-32R1  \n",
    " \n",
    "上記レポートの勝手な要約。\n",
    "- BNSでフィーチャー･スケーリングする場合フィーチャー数を減らす必要は無い  \n",
    "- 減らす場合はInformation Gainで選択し、BNSでスケーリングするのが良い  \n",
    "\n",
    "1点目が良い。更に、以下の特長も良い。\n",
    "\n",
    "- 実装は楽(上記レポートのp2の右上でBNS関連の７行を実装できれば良い)\n",
    "\n",
    "## 前提  \n",
    "言語処理100本ノックをベースとするのでLogisticRegressionを使用してみる。  \n",
    "\n",
    "また、以下の処理結果を比較。  \n",
    "1. フィーチャーを抽出して、あるフィーチャーがあるレコードに登場する回数でスケーリング。上記の通りLogisticRegressionで処理(基本的な手法)  \n",
    "1. 1.と同じフィーチャーを使い、BNSでスケーリング。 1.と同じLogisticRegressionで処理(BNSで処理)  \n",
    "\n",
    "## 結果\n",
    "結果は上から accuracy_score()、confusion_matrix(),classification_report()の出力。accuracyベースで7%の向上。日本語で処理した際程差がつかなかった。。。思いつく違いは以下の通り。  \n",
    "- データ自体  \n",
    "- データ数  \n",
    "- ポジ/ネガの割合。前回はポジティブデータの割合は少ない。今回はほぼ５割(上記レポートでも、ポジティブデータは通常少ない、という記載がある)。  \n",
    "- トレーニング用データとテスト用データの割合(今回は8:2。前回は9:1)\n",
    "\n",
    "BNSケースも良くはないが今回はBNSの効果検証のためここまで。\n",
    "\n",
    "なお、データ数を少なくして処理をしてみたがその際の結果は、基本的な手法の精度が上がり、BNSはこれを上回ったが＋２%程度であった。\n",
    "\n",
    "### 基本的な手法での結果\n",
    "```\n",
    "0.7525\n",
    " [[810 267]\n",
    " [261 795]]\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "   positive       0.76      0.75      0.75      1077\n",
    "   negative       0.75      0.75      0.75      1056\n",
    "\n",
    "avg / total       0.75      0.75      0.75      2133\n",
    "```\n",
    "### BNSでの処理結果\n",
    "```\n",
    "0.8223\n",
    " [[885 192]\n",
    " [187 869]]\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "   positive       0.83      0.82      0.82      1077\n",
    "   negative       0.82      0.82      0.82      1056\n",
    "\n",
    "avg / total       0.82      0.82      0.82      2133\n",
    "```\n",
    "### TF-IDFでの処理結果\n",
    "基本的な手法とBNSのケースは同じシャッフル結果を使用しているが、TF-IDFは異なるシャッフル結果を使用している。\n",
    "```\n",
    "0.7843\n",
    " [[851 226]\n",
    " [234 822]]\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "   positive       0.78      0.79      0.79      1077\n",
    "   negative       0.78      0.78      0.78      1056\n",
    "\n",
    "avg / total       0.78      0.78      0.78      2133\n",
    "```\n",
    "## その他  \n",
    "お世話になったサイト。\n",
    "- [言語処理100本ノック 2015][2]\n",
    "- 言語処理100本ノックで使われているデータ。Bo Pang氏とLillian Lee氏が公開しているMovie Review Dataのsentence polarity dataset v1.0が使われている。サイトは[こちら][3]\n",
    "- [scikitの様々なページ][4] \n",
    "- [githubのscikitのページ][5]。ストップワードはここから。  \n",
    " \n",
    "[1]: http://www.hpl.hp.com/techreports/2007/HPL-2007-32R1.pdf?ref=driverlayer.com/web\n",
    "[2]: http://www.cl.ecei.tohoku.ac.jp/nlp100/#ch8 \n",
    "[3]: http://www.cs.cornell.edu/people/pabo/movie-review-data/ \n",
    "[4]: http://scikit-learn.org/stable/index.html\n",
    "[5]: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前処理\n",
    "## 文字コード確認と変換\n",
    "久しぶりなので文字コードの確認から。その後iconvを使ってUTF-8に変換する。head/tailでおざなりだが確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.644800Z",
     "start_time": "2018-08-11T08:47:56.626011Z"
    }
   },
   "outputs": [],
   "source": [
    "#!chardetect ./rt-polarity/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.668438Z",
     "start_time": "2018-08-11T08:47:56.662658Z"
    }
   },
   "outputs": [],
   "source": [
    "#!iconv -c -s --verbose -f WINDOWS-1252 -t UTF-8 ./rt-polarity/rt-polarity.pos > ./rt-polarity/rt-polarity.pos.utf8\n",
    "#!iconv -c -s --verbose -f WINDOWS-1252 -t UTF-8 ./rt-polarity/rt-polarity.neg > ./rt-polarity/rt-polarity.neg.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.695021Z",
     "start_time": "2018-08-11T08:47:56.689504Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!chardetect ./rt-polarity/*.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.743873Z",
     "start_time": "2018-08-11T08:47:56.738207Z"
    }
   },
   "outputs": [],
   "source": [
    "#!head ./rt-polarity/rt-polarity.pos.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.771333Z",
     "start_time": "2018-08-11T08:47:56.766052Z"
    }
   },
   "outputs": [],
   "source": [
    "#!head ./rt-polarity/rt-polarity.neg.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.787691Z",
     "start_time": "2018-08-11T08:47:56.778247Z"
    }
   },
   "outputs": [],
   "source": [
    "#!tail ./rt-polarity/rt-polarity.pos.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:56.811296Z",
     "start_time": "2018-08-11T08:47:56.805546Z"
    }
   },
   "outputs": [],
   "source": [
    "#!tail ./rt-polarity/rt-polarity.neg.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストデータの事前処理\n",
    "1. テキストデータのロード \n",
    "1. y(正解データ)準備\n",
    "1. checkWord()でクリーニング。\n",
    "1. stopwordsを処理\n",
    "1. stemming処理\n",
    "\n",
    "lower()は念の為。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:57.224655Z",
     "start_time": "2018-08-11T08:47:56.853087Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkWord()の処理 \n",
    "- アルファベットだけ。そのまま返す\n",
    "- 記号1文字。抜く。\n",
    "- 記号(アルファベット以外のキャラクター)始まりでアルファベット。記号を抜く\n",
    "- アルファベット始まりで記号終わり。記号を抜く\n",
    "- 's。'sを抜く。\n",
    "- 'd。'dを抜く。\n",
    "- 'm。'mを抜く。。\n",
    "- 've。'veを抜く。。\n",
    "- 're。'reを抜く。。\n",
    "- 'll。'llを抜く。。\n",
    "- n't。そのまま。doesn'tとかそのまま。\n",
    "- director/co-writerもそのまま。/は抜きたいが-は抜きたくない。しかしルールなさそう。\n",
    "- その他気になりつつも抜かないもの。 \n",
    "d'etre fun-for-fun's-sake cam'ron children's-movie it's-surreal bull's-eye so-bad-it's-funny life-at-arm's-length early-'80s o'neill mid-'70s c'mon matrix'-style stalk'n'slash sub'-standard soul's-eye so-bad-it's-good o'fallon so-bad-they're-good pasach'ke show-don't-tell o'clock can't-miss shoot-'em-up\n",
    "\n",
    "まだ抜いた方が良さそうなものもあるが数は少なそうなので放置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:57.282274Z",
     "start_time": "2018-08-11T08:47:57.228920Z"
    }
   },
   "outputs": [],
   "source": [
    "def checkWord(_l):\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    retword=\"\"\n",
    "    alphaonly = r'^[a-zA-Z]+$'\n",
    "    singlesymbol = r'^[^a-zA-Z]$'\n",
    "    alphasecond = r'^([^a-zA-Z]+)'\n",
    "    alphafirst = r'[^a-zA-Z]+$'\n",
    "    lasts = r'\\'s$'\n",
    "    lastd = r'\\'d$'\n",
    "    lastm = r'\\'m$'\n",
    "    lastve = r'\\'ve$'\n",
    "    lastre = r'\\'re$'\n",
    "    lastll = r'\\'ll$'\n",
    "    \n",
    "    for _w in _l:\n",
    "        if re.match(alphaonly, _w):\n",
    "            retword = _w\n",
    "        elif re.match(singlesymbol,_w):\n",
    "            retword = \"\"\n",
    "        else:\n",
    "            retword = re.sub(alphasecond,\"\",_w)\n",
    "            retword = re.sub(alphafirst,\"\",retword)\n",
    "            retword = re.sub(lasts,\"\",retword)\n",
    "            retword = re.sub(lastd,\"\",retword)\n",
    "            retword = re.sub(lastm,\"\",retword)\n",
    "            retword = re.sub(lastve,\"\",retword)\n",
    "            retword = re.sub(lastre,\"\",retword)\n",
    "            retword = re.sub(lastll,\"\",retword)\n",
    "        yield retword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのロードと正解データの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:57.572168Z",
     "start_time": "2018-08-11T08:47:57.297998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posdata = np.loadtxt(\"./rt-polarity/rt-polarity.pos.utf8\",delimiter=\"\\n\",dtype=str)\n",
    "negdata = np.loadtxt(\"./rt-polarity/rt-polarity.neg.utf8\",delimiter=\"\\n\",dtype=str)\n",
    "y_pos = np.ones((posdata.shape[0]))\n",
    "y_neg = np.zeros((negdata.shape[0]))\n",
    "\n",
    "\n",
    "# posdata = np.loadtxt(\"./rt-polarity/rt-polarity.pos.utf8\",delimiter=\"\\n\",dtype=str)\n",
    "# negdata = np.loadtxt(\"./rt-polarity/rt-polarity.neg.utf8\",delimiter=\"\\n\",dtype=str)\n",
    "# # posdata = posdata[0:140]\n",
    "# # negdata = negdata[0:700]\n",
    "# posdata = shuffle(posdata, n_samples=140, random_state=0)\n",
    "# negdata = shuffle(negdata, n_samples=700, random_state=0)\n",
    "# y_pos = np.ones((posdata.shape[0]))\n",
    "# y_neg = np.zeros((negdata.shape[0]))\n",
    "# print(\"{} {}\".format(posdata.shape,negdata.shape))\n",
    "\n",
    "# import math\n",
    "# posdata = np.loadtxt(\"./rt-polarity/rt-polarity.pos.utf8\",delimiter=\"\\n\",dtype=str)\n",
    "# negdata = np.loadtxt(\"./rt-polarity/rt-polarity.neg.utf8\",delimiter=\"\\n\",dtype=str)\n",
    "# y_neg = np.zeros((negdata.shape[0]))\n",
    "# numP = math.ceil( y_neg.shape[0]*0.2 )\n",
    "# posdata = posdata[0:numP]\n",
    "# y_pos = np.ones((posdata.shape[0]))\n",
    "# posdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkWordの呼び出し=クリーニング\n",
    "Xとなるデータはここではリストに入れておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:47:59.387526Z",
     "start_time": "2018-08-11T08:47:57.576529Z"
    }
   },
   "outputs": [],
   "source": [
    "filtereddata = [[],[]]\n",
    "\n",
    "for _i,_d in  enumerate([posdata,negdata]):\n",
    "    for _l in _d:\n",
    "        _l=_l.lower()\n",
    "        _l=_l.split()\n",
    "        _ol = \"\"\n",
    "        _ol = [w for w in checkWord(_l) if w != \"\"]\n",
    "        _ol = \" \".join(_ol)\n",
    "        filtereddata[_i].append(_ol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワード処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:01.488738Z",
     "start_time": "2018-08-11T08:47:59.398258Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "stopwords=[\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "afterStopWords = [[],[]]\n",
    "for _n,_d in  enumerate(filtereddata):\n",
    "    for _l in _d:\n",
    "        afterStopWords[_n].append( [w for w in _l.split() if not w in stopwords] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステミング処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:11.593992Z",
     "start_time": "2018-08-11T08:48:01.492893Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "\n",
    "stemmedWords=[[],[]]\n",
    "stemmer = stem.PorterStemmer()\n",
    "\n",
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "for _n,_d in  enumerate(afterStopWords):\n",
    "    for _l in _d:\n",
    "        stemmedWords[_n].append( [stemmer.stem(w) for w in _l] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reストップワード処理\n",
    "再びストップワード処理。\n",
    "これまでの流れだと、例えば\"gives\"はストップワード処理に該当しない。その上でステミングで\"give\"になる。\"give\"はストップワードにリストされているのでもう一度処理しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:12.989595Z",
     "start_time": "2018-08-11T08:48:11.598082Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "re_afterStopWords = [[],[]]\n",
    "for _n,_d in  enumerate(stemmedWords):\n",
    "    for _l in _d:\n",
    "        re_afterStopWords[_n].append( [w for w in _l if not w in stopwords] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:13.589501Z",
     "start_time": "2018-08-11T08:48:13.002026Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmedWords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのシャッフルと分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:13.600676Z",
     "start_time": "2018-08-11T08:48:13.594514Z"
    }
   },
   "outputs": [],
   "source": [
    "data = re_afterStopWords[0] + re_afterStopWords[1]\n",
    "y_pos_neg = np.r_[y_pos,y_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:13.631418Z",
     "start_time": "2018-08-11T08:48:13.605469Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(data,y_pos_neg, random_state=20180729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:13.659957Z",
     "start_time": "2018-08-11T08:48:13.638487Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:13.680490Z",
     "start_time": "2018-08-11T08:48:13.669983Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"{} {} {} {}\".format(len(X_train_txt),len(X_test_txt),y_train.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本的な手法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フィーチャー抽出\n",
    "X_train_txtを対象にフィーチャー抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:28.890067Z",
     "start_time": "2018-08-11T08:48:13.690047Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "words=[]\n",
    "for _n,_l in enumerate(X_train_txt):\n",
    "    words = words + [w for w in _l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重複排除\n",
    "- フィーチャー作成 \n",
    "- フィーチャーを減らすことは考えない。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:29.169610Z",
     "start_time": "2018-08-11T08:48:28.904621Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "c = Counter(words)\n",
    "c.most_common()\n",
    "len(c.most_common())\n",
    "features=list(c)\n",
    "len(features)\n",
    "# features\n",
    "print(\"{} {} {}\".format(len(features),len(X_train_txt),len(X_test_txt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T00:02:05.628074Z",
     "start_time": "2018-08-05T00:02:05.617670Z"
    }
   },
   "source": [
    "## データ作成\n",
    "### データ保持用アレイ作成\n",
    "### トレーニング用データとテスト用データ作成\n",
    "出現回数をフィーチャーの値として使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:48.189165Z",
     "start_time": "2018-08-11T08:48:29.174345Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "X_train = np.zeros((len(X_train_txt), len(features)))\n",
    "X_test = np.zeros((len(X_test_txt), len(features)))\n",
    "_X = [X_train,X_test]\n",
    "for _i,_d in  enumerate([X_train_txt,X_test_txt]):\n",
    "    for _n,_l in enumerate(_d):\n",
    "        for _w in _l:\n",
    "            try:\n",
    "                _X[_i][_n][features.index(_w)] += 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "#                 print(\"_i:{} _n:{} _l:{} _w:{}\".format(_i,_n,_l,_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:48.213846Z",
     "start_time": "2018-08-11T08:48:48.194142Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearnでLogisticRegression\n",
    "### sklearn関連関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:48.232850Z",
     "start_time": "2018-08-11T08:48:48.225978Z"
    }
   },
   "outputs": [],
   "source": [
    "def sklearnLR(_X_train,_y_train):\n",
    "    from sklearn import linear_model\n",
    "    # logreg = linear_model.LogisticRegressionCV()\n",
    "    _model = linear_model.LogisticRegression()\n",
    "    _model.fit(_X_train, _y_train)\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:48.255948Z",
     "start_time": "2018-08-11T08:48:48.241600Z"
    }
   },
   "outputs": [],
   "source": [
    "def sklearnScorePredict(_model,_X_test,_y_test):\n",
    "    _score=_model.score(_X_test,_y_test)\n",
    "    _pred=_model.predict(_X_test)\n",
    "    return _pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:48.282671Z",
     "start_time": "2018-08-11T08:48:48.266065Z"
    }
   },
   "outputs": [],
   "source": [
    "def sklearnCheckResult(_y_test,_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"{:.4f}\\n {}\".format(\n",
    "        accuracy_score(_y_test, _pred),\n",
    "        confusion_matrix(_y_test, _pred)\n",
    "    ))\n",
    "    target_names = ['positive', 'negative']\n",
    "    print(classification_report(_y_test, _pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:49.355059Z",
     "start_time": "2018-08-11T08:48:48.296975Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = sklearnLR(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:49.551004Z",
     "start_time": "2018-08-11T08:48:49.371733Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = sklearnScorePredict(logreg,X_test,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:49.577658Z",
     "start_time": "2018-08-11T08:48:49.555249Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearnCheckResult(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T00:24:24.006142Z",
     "start_time": "2018-08-05T00:24:23.985869Z"
    }
   },
   "source": [
    "# BNSで処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T00:23:47.826323Z",
     "start_time": "2018-08-05T00:23:47.818183Z"
    }
   },
   "source": [
    "## メモ\n",
    "$\n",
    "\\begin{align}\n",
    "|F^{-1}(tpr)-F^{-1}(fpr)| \\\\\n",
    "tpr = tp/pos \\\\\n",
    "fpr = fp/neg\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "pos ポジティブのトレーニングケース数  \n",
    "neg ネガティブのトレーニングケース数  \n",
    "tp あるワードを含むポジティブトレーニングケース  \n",
    "fp あるワードを含むネガティブトレーニングケース  \n",
    "\n",
    "stemmedWords[0]にポジティブのトレーニングデータ。  \n",
    "stemmedWords[1]にネガティブのトレーニングデータ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングデータ数確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:48:49.597629Z",
     "start_time": "2018-08-11T08:48:49.583605Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos = sum(y_train)\n",
    "neg = y_train.shape[0] - pos\n",
    "print(\"{} = {} + {} \".format(y_train.shape[0],pos,neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:09:14.698497Z",
     "start_time": "2018-08-06T14:09:14.688415Z"
    }
   },
   "source": [
    "## TPとFP算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:50:05.548917Z",
     "start_time": "2018-08-11T08:48:49.602946Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "tplist=[]\n",
    "fplist=[]\n",
    "worklist=[]\n",
    "# worklist = [1 for _r in re_afterStopWords[0] if \"kafka\" in _r]\n",
    "for _n,_w in enumerate(features):\n",
    "    worklist = [1 for _r in re_afterStopWords[0] if _w in _r]\n",
    "#     worklist = [1 for _r in X_train_txt if _w in _r]\n",
    "    tplist.append(sum(worklist))\n",
    "    worklist = [1 for _r in re_afterStopWords[1] if _w in _r]\n",
    "#     worklist = [1 for _r in X_test_txt if _w in _r]\n",
    "    fplist.append(sum(worklist))\n",
    "    if _n % 400 == 0:\n",
    "        print(\"{} {} {} {}\".format(_n,_w,tplist[_n],fplist[_n]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPRとFPR算出\n",
    "TPFとFPRを算出  \n",
    "- tpf(or fpr)の値が1だと後続でエラー(ppt()で-inf)となるのでtpr値=pos数の場合tprを-0.00001しておく(ある単語が全レコードにあるとこのケースに該当する。極めて稀なケースだが念の為)。\n",
    "- tpf(or fpr)の値が0だと後続でエラー(同上)となるので0より大きい値を代入。tprlistでは0より大きい値は0.00023391812865497077なので0.00001を代入する。\n",
    "- tmpが並ぶのが恥ずかしいが。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:50:05.645865Z",
     "start_time": "2018-08-11T08:50:05.553155Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp1 = [x if x!=pos else x-0.00001 for x in tplist]\n",
    "tmp2 = [x if x!=neg else x-0.000011 for x in fplist]\n",
    "tmp3 = list(map(lambda x: x/pos,tmp1))\n",
    "tmp4 = list(map(lambda x: x/neg,tmp2))\n",
    "tprlist  = [max(0.00001,x) for x in tmp3]\n",
    "fprlist  = [max(0.00001,x) for x in tmp4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNS値算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:50:20.739067Z",
     "start_time": "2018-08-11T08:50:05.650417Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "tmp7 = [norm.ppf(x)-norm.ppf(y) for x,y in zip(tprlist,fprlist)]\n",
    "bns = list(map(lambda x: abs(x),tmp7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNSスケーリングのもとでLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:50:53.828192Z",
     "start_time": "2018-08-11T08:50:20.743454Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "B_X_train = np.zeros((len(X_train_txt), len(features)))\n",
    "B_X_test = np.zeros((len(X_test_txt), len(features)))\n",
    "_X = [B_X_train,B_X_test]\n",
    "for _i,_d in  enumerate([X_train_txt,X_test_txt]):\n",
    "    for _n,_l in enumerate(_d):\n",
    "        for _w in _l:\n",
    "            try:\n",
    "                _X[_i][_n][features.index(_w)] = bns[features.index(_w)]\n",
    "            except ValueError:\n",
    "#                 print(\"_i:{} _n:{} _l:{} _w:{}\".format(_i,_n,_l,_w))\n",
    "#                 print(\"_i:{} _n:{} _w:{}\".format(_i,_n,_w))\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:50:55.067995Z",
     "start_time": "2018-08-11T08:50:53.834479Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B_logreg = sklearnLR(B_X_train,y_train)\n",
    "B_pred = sklearnScorePredict(B_logreg,B_X_test,y_test) \n",
    "sklearnCheckResult(y_test,B_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFで処理\n",
    "scikitの[Working With Text Data][0]に記載されたコード。\n",
    "\n",
    "[0]: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T08:50:56.938881Z",
     "start_time": "2018-08-11T08:50:55.076886Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tfi_data = np.r_[posdata,negdata]\n",
    "tfi_y_pos_neg = np.r_[y_pos,y_neg]\n",
    "\n",
    "tfi_X, tfi_y = shuffle(tfi_data,tfi_y_pos_neg, random_state=20180729)\n",
    "tfi_X_train_txt, tfi_X_test_txt, tfi_y_train, tfi_y_test = train_test_split(tfi_X, tfi_y, test_size=0.2, random_state=42)\n",
    "tfi_X_train_txt[0]\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(tfi_X_train_txt)\n",
    "# X_train_counts.shape\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, tfi_y_train)\n",
    "\n",
    "X_new_counts = count_vect.transform(tfi_X_test_txt)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "tfi_predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "sklearnCheckResult(y_test,tfi_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

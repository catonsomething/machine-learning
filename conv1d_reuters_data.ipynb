{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv1d_reuters_data\n",
    "- 「[PythonとKerasによるディープラーニング][4]」第6章を参考に、kerasの[examples][1] [reuters_mlp.py][2]のデータにconv1dを適用してみる。\n",
    "- reutersデータにはfasttext([imdb_fasttext.py][3])を適用する。\n",
    "[1]: https://github.com/keras-team/keras/tree/master/examples\n",
    "[2]: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "[3]: https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py\n",
    "[4]: https://book.mynavi.jp/ec/products/detail/id=90124\n",
    "## 目標と結果\n",
    "### 目標\n",
    "目標はfasttext_reuters_dataと同様にテストデータでのaccuracy=0.8を目指す。  \n",
    "### 結果\n",
    "epoch 10/10で、loss: 0.0599 - acc: 0.9841 - val_loss: 0.0596 - val_acc: 0.9847となり、テストデータではloss 0.06、acc 0.98。\n",
    "\n",
    "## 変更点\n",
    "大きな変更なない。\n",
    "## その他\n",
    "### 結果について\n",
    "不安になるような結果。\n",
    "1. 出だしからacc/val_acc共に0.9783\n",
    "1. 10回のepochでacc系は微増しloss系は微減し続ける\n",
    "1. テストデータも同じような結果  \n",
    "\n",
    "心配なので連続でfasttext_reuters_dataを実行してみるがここは変わらず。データ件数も正しく表示されているので大丈夫なのかと(Train on 7185 samples, validate on 1797 samples)。まずCNN、改善のためCNNに加えてRNNを適用と考えていたので拍子抜け。。。\n",
    "## 軽量\n",
    "「[PythonとKerasによるディープラーニング][4]」第6章のどこかに「軽量」と書かれていたが確かに軽量。最終的にcolabで実行したがPCでも耐えられるレベル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:29:19.009050Z",
     "start_time": "2018-09-24T00:29:15.564532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''This example demonstrates the use of fasttext for text classification\n",
    "Based on Joulin et al's paper:\n",
    "Bags of Tricks for Efficient Text Classification\n",
    "https://arxiv.org/abs/1607.01759\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
    "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "# max_features = 20000\n",
    "max_features = 2500\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:29:36.045424Z",
     "start_time": "2018-09-24T00:29:35.357258Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "max_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:29:52.324510Z",
     "start_time": "2018-09-24T00:29:52.284681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 train sequences\n",
      "2246 test sequences\n",
      "Average train sequence length: 145\n",
      "Average test sequence length: 147\n"
     ]
    }
   ],
   "source": [
    "l_x_train = len(x_train)\n",
    "l_x_test = len(x_test)\n",
    "m_x_train =  np.mean(list(map(len, x_train)), dtype=int)\n",
    "m_x_test =  np.mean(list(map(len, x_test)), dtype=int)\n",
    "\n",
    "print(l_x_train, 'train sequences')\n",
    "print(l_x_test, 'test sequences')\n",
    "print('Average train sequence length: {}'.format(m_x_train))\n",
    "print('Average test sequence length: {}'.format(m_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:30:13.664479Z",
     "start_time": "2018-09-24T00:30:10.797430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "max_features before np.max() 2500\n",
      "max_features after  np.max() 93394\n",
      "Average train sequence length: 290\n",
      "Average test sequence length: 289\n",
      "CPU times: user 2.83 s, sys: 15.6 ms, total: 2.85 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    print(\"max_features before np.max()\",max_features)\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "    print(\"max_features after  np.max()\",max_features)\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:30:44.196254Z",
     "start_time": "2018-09-24T00:30:43.759069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (8982, 400)\n",
      "x_test shape: (2246, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:30:56.737825Z",
     "start_time": "2018-09-24T00:30:56.723545Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T00:44:37.384710Z",
     "start_time": "2018-09-24T00:35:45.720716Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 128)          11954432  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 394, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 78, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 72, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                1518      \n",
      "=================================================================\n",
      "Total params: 11,991,854\n",
      "Trainable params: 11,991,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/10\n",
      "7185/7185 [==============================] - 51s 7ms/step - loss: 0.1034 - acc: 0.9783 - val_loss: 0.1016 - val_acc: 0.9783\n",
      "Epoch 2/10\n",
      "7185/7185 [==============================] - 59s 8ms/step - loss: 0.0988 - acc: 0.9783 - val_loss: 0.0947 - val_acc: 0.9783\n",
      "Epoch 3/10\n",
      "7185/7185 [==============================] - 66s 9ms/step - loss: 0.0894 - acc: 0.9783 - val_loss: 0.0841 - val_acc: 0.9783\n",
      "Epoch 4/10\n",
      "7185/7185 [==============================] - 63s 9ms/step - loss: 0.0803 - acc: 0.9783 - val_loss: 0.0771 - val_acc: 0.9783\n",
      "Epoch 5/10\n",
      "7185/7185 [==============================] - 48s 7ms/step - loss: 0.0749 - acc: 0.9784 - val_loss: 0.0733 - val_acc: 0.9822\n",
      "Epoch 6/10\n",
      "7185/7185 [==============================] - 55s 8ms/step - loss: 0.0716 - acc: 0.9820 - val_loss: 0.0702 - val_acc: 0.9830\n",
      "Epoch 7/10\n",
      "7185/7185 [==============================] - 50s 7ms/step - loss: 0.0686 - acc: 0.9835 - val_loss: 0.0673 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      "7185/7185 [==============================] - 50s 7ms/step - loss: 0.0656 - acc: 0.9836 - val_loss: 0.0645 - val_acc: 0.9835\n",
      "Epoch 9/10\n",
      "7185/7185 [==============================] - 48s 7ms/step - loss: 0.0629 - acc: 0.9837 - val_loss: 0.0622 - val_acc: 0.9836\n",
      "Epoch 10/10\n",
      "7185/7185 [==============================] - 40s 6ms/step - loss: 0.0608 - acc: 0.9839 - val_loss: 0.0604 - val_acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "embedding_dims=128 # default value of the original statement\n",
    "\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\n",
    "      \"batch_size: \", 128, \"epochs: \", 10)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "# model.add(layers.Dense(1))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:24:32.871463Z",
     "start_time": "2018-09-24T08:24:32.867892Z"
    }
   },
   "source": [
    "# 念の為"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=1472; epochs=5\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "#           validation_data=(x_test, y_test))\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果\n",
    "## conv1d\n",
    "```\n",
    "max_features:  93394 embedding_dims:  128 input_length:  400 batch_size:  128 epochs:  10\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 400, 128)          11954432  \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 394, 32)           28704     \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 78, 32)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 72, 32)            7200      \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 46)                1518      \n",
    "=================================================================\n",
    "Total params: 11,991,854\n",
    "Trainable params: 11,991,854\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 7185 samples, validate on 1797 samples\n",
    "Epoch 1/10\n",
    "7185/7185 [==============================] - 4s 517us/step - loss: 0.1020 - acc: 0.9783 - val_loss: 0.0992 - val_acc: 0.9783\n",
    "Epoch 2/10\n",
    "7185/7185 [==============================] - 2s 224us/step - loss: 0.0951 - acc: 0.9783 - val_loss: 0.0897 - val_acc: 0.9783\n",
    "Epoch 3/10\n",
    "7185/7185 [==============================] - 2s 225us/step - loss: 0.0837 - acc: 0.9783 - val_loss: 0.0786 - val_acc: 0.9783\n",
    "Epoch 4/10\n",
    "7185/7185 [==============================] - 2s 224us/step - loss: 0.0759 - acc: 0.9783 - val_loss: 0.0740 - val_acc: 0.9783\n",
    "Epoch 5/10\n",
    "7185/7185 [==============================] - 2s 224us/step - loss: 0.0724 - acc: 0.9794 - val_loss: 0.0707 - val_acc: 0.9822\n",
    "Epoch 6/10\n",
    "7185/7185 [==============================] - 2s 226us/step - loss: 0.0690 - acc: 0.9828 - val_loss: 0.0674 - val_acc: 0.9833\n",
    "Epoch 7/10\n",
    "7185/7185 [==============================] - 2s 224us/step - loss: 0.0658 - acc: 0.9835 - val_loss: 0.0645 - val_acc: 0.9835\n",
    "Epoch 8/10\n",
    "7185/7185 [==============================] - 2s 223us/step - loss: 0.0633 - acc: 0.9836 - val_loss: 0.0624 - val_acc: 0.9835\n",
    "Epoch 9/10\n",
    "7185/7185 [==============================] - 2s 226us/step - loss: 0.0614 - acc: 0.9837 - val_loss: 0.0609 - val_acc: 0.9838\n",
    "Epoch 10/10\n",
    "7185/7185 [==============================] - 2s 225us/step - loss: 0.0599 - acc: 0.9841 - val_loss: 0.0596 - val_acc: 0.9847\n",
    "2246/2246 [==============================] - 0s 107us/step\n",
    "Test score: 0.05984580424595176\n",
    "Test accuracy: 0.9848135746915958\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

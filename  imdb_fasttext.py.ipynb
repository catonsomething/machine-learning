{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imdb_fasttext.py\n",
    "[imdb_fasttext.py][1]  \n",
    "\n",
    "[1]: https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T01:00:05.672034Z",
     "start_time": "2018-09-16T01:00:05.585077Z"
    }
   },
   "outputs": [],
   "source": [
    "'''This example demonstrates the use of fasttext for text classification\n",
    "Based on Joulin et al's paper:\n",
    "Bags of Tricks for Efficient Text Classification\n",
    "https://arxiv.org/abs/1607.01759\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
    "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T01:00:25.494149Z",
     "start_time": "2018-09-16T01:00:06.665020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T01:00:31.986741Z",
     "start_time": "2018-09-16T01:00:31.698011Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle_file=\"./imdb_fasttext.pickle\"\n",
    "\n",
    "# with open(pickle_file, 'wb') as f:\n",
    "#     pickle.dump(x_train, f)\n",
    "#     pickle.dump(y_train, f)\n",
    "#     pickle.dump(x_test, f)\n",
    "#     pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T01:00:34.408735Z",
     "start_time": "2018-09-16T01:00:33.362663Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file=\"./imdb_fasttext.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "    y_train = pickle.load(f)\n",
    "    x_test = pickle.load(f)\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T01:00:34.925118Z",
     "start_time": "2018-09-16T01:00:34.870405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T01:01:42.269690Z",
     "start_time": "2018-09-16T01:00:36.818684Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 428\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "  736/25000 [..............................] - ETA: 20:57 - loss: 0.6926 - acc: 0.5503"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果\n",
    "## imdb_fasttext.py(オリジナル。ngram=1)\n",
    "acc:92.5% & val_acc:88.8%  total: 4min 49s\n",
    "## imdb_fasttext.py(オリジナル。ngram=2)\n",
    "acc:99.5% & val_acc:90.4%  total: 3h 4s\n",
    "## imdb_fasttext.py(colab。ngram=2)\n",
    "acc:99.5% & val_acc:90.4%  total:5m 5s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  110894 embedding_dims:  50 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 4s 401us/step - loss: 2.8477 - acc: 0.3599 - val_loss: 2.2456 - val_acc: 0.3980\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 3s 347us/step - loss: 2.1460 - acc: 0.3957 - val_loss: 2.0688 - val_acc: 0.4239\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 3s 361us/step - loss: 1.9780 - acc: 0.4505 - val_loss: 1.9322 - val_acc: 0.4835\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 3s 361us/step - loss: 1.8313 - acc: 0.5137 - val_loss: 1.8073 - val_acc: 0.5508\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 3s 356us/step - loss: 1.6894 - acc: 0.5916 - val_loss: 1.6923 - val_acc: 0.6046\n",
    "<keras.callbacks.History at 0x7f2b21207320>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=92\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  110894 embedding_dims:  92 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 5s 560us/step - loss: 2.6932 - acc: 0.3784 - val_loss: 2.1609 - val_acc: 0.4127\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 5s 507us/step - loss: 2.0320 - acc: 0.4378 - val_loss: 1.9462 - val_acc: 0.4728\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 5s 517us/step - loss: 1.8201 - acc: 0.5331 - val_loss: 1.7741 - val_acc: 0.5761\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 5s 526us/step - loss: 1.6302 - acc: 0.6240 - val_loss: 1.6287 - val_acc: 0.6327\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 5s 528us/step - loss: 1.4653 - acc: 0.6716 - val_loss: 1.5118 - val_acc: 0.6621\n",
    "<keras.callbacks.History at 0x7f2b232df1d0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=184\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  110894 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 9s 970us/step - loss: 2.5318 - acc: 0.3890 - val_loss: 2.0511 - val_acc: 0.4479\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 8s 865us/step - loss: 1.8905 - acc: 0.5070 - val_loss: 1.7928 - val_acc: 0.5712\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 8s 867us/step - loss: 1.6199 - acc: 0.6264 - val_loss: 1.5926 - val_acc: 0.6438\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 8s 866us/step - loss: 1.4012 - acc: 0.6818 - val_loss: 1.4466 - val_acc: 0.6687\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 8s 872us/step - loss: 1.2287 - acc: 0.7147 - val_loss: 1.3363 - val_acc: 0.6910\n",
    "<keras.callbacks.History at 0x7f2b22c4fcf8>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=184\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 2.5311 - acc: 0.3899 - val_loss: 2.0695 - val_acc: 0.4417\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 104s 12ms/step - loss: 1.9066 - acc: 0.4923 - val_loss: 1.8054 - val_acc: 0.5654\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.6344 - acc: 0.6181 - val_loss: 1.6014 - val_acc: 0.6345\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.4095 - acc: 0.6777 - val_loss: 1.4489 - val_acc: 0.6719\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 1.2326 - acc: 0.7144 - val_loss: 1.3383 - val_acc: 0.6901\n",
    "<keras.callbacks.History at 0x7f5fa6702470>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "max_features:  93394 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 2.5311 - acc: 0.3899 - val_loss: 2.0695 - val_acc: 0.4417\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 104s 12ms/step - loss: 1.9066 - acc: 0.4923 - val_loss: 1.8054 - val_acc: 0.5654\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.6344 - acc: 0.6181 - val_loss: 1.6014 - val_acc: 0.6345\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.4095 - acc: 0.6777 - val_loss: 1.4489 - val_acc: 0.6719\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 1.2326 - acc: 0.7144 - val_loss: 1.3383 - val_acc: 0.6901\n",
    "<keras.callbacks.History at 0x7f5fa6702470>\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  5\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_2 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_average_pooling1d_2 ( (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.3918 - acc: 0.4105 - val_loss: 1.9521 - val_acc: 0.4884\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.7406 - acc: 0.5765 - val_loss: 1.6333 - val_acc: 0.6269\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4137 - acc: 0.6798 - val_loss: 1.4235 - val_acc: 0.6750\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.1830 - acc: 0.7267 - val_loss: 1.2888 - val_acc: 0.7008\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.0108 - acc: 0.7735 - val_loss: 1.1930 - val_acc: 0.7302\n",
    "<keras.callbacks.History at 0x7f1d5259ec50>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=368\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  5\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_5 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.2130 - acc: 0.4660 - val_loss: 1.7213 - val_acc: 0.5726\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4644 - acc: 0.6672 - val_loss: 1.3551 - val_acc: 0.6848\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 13s 1ms/step - loss: 1.0976 - acc: 0.7552 - val_loss: 1.1304 - val_acc: 0.7453\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.8324 - acc: 0.8201 - val_loss: 0.9905 - val_acc: 0.7725\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.6292 - acc: 0.8595 - val_loss: 0.9165 - val_acc: 0.7845\n",
    "<keras.callbacks.History at 0x7f1d18481908>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=368; epochs=10\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  10\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_7 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_3 (Glob (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/10\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.2225 - acc: 0.4787 - val_loss: 1.7340 - val_acc: 0.5557\n",
    "Epoch 2/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4701 - acc: 0.6730 - val_loss: 1.3616 - val_acc: 0.6906\n",
    "Epoch 3/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.0971 - acc: 0.7561 - val_loss: 1.1406 - val_acc: 0.7413\n",
    "Epoch 4/10\n",
    "8982/8982 [==============================] - 13s 1ms/step - loss: 0.8271 - acc: 0.8169 - val_loss: 0.9997 - val_acc: 0.7783\n",
    "Epoch 5/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.6232 - acc: 0.8559 - val_loss: 0.9194 - val_acc: 0.7832\n",
    "Epoch 6/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.4669 - acc: 0.8967 - val_loss: 0.8753 - val_acc: 0.7925\n",
    "Epoch 7/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.3441 - acc: 0.9255 - val_loss: 0.8531 - val_acc: 0.7956\n",
    "Epoch 8/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.2522 - acc: 0.9432 - val_loss: 0.8475 - val_acc: 0.7979\n",
    "Epoch 9/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1897 - acc: 0.9532 - val_loss: 0.8557 - val_acc: 0.8001\n",
    "Epoch 10/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1489 - acc: 0.9548 - val_loss: 0.8640 - val_acc: 0.7983\n",
    "<keras.callbacks.History at 0x7f1d181039b0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=368; epochs=20\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  20\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_8 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_4 (Glob (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_6 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/20\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.2099 - acc: 0.4986 - val_loss: 1.7325 - val_acc: 0.5508\n",
    "Epoch 2/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4735 - acc: 0.6650 - val_loss: 1.3595 - val_acc: 0.6923\n",
    "Epoch 3/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.1021 - acc: 0.7585 - val_loss: 1.1299 - val_acc: 0.7467\n",
    "Epoch 4/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.8327 - acc: 0.8174 - val_loss: 0.9942 - val_acc: 0.7765\n",
    "Epoch 5/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.6300 - acc: 0.8577 - val_loss: 0.9140 - val_acc: 0.7827\n",
    "Epoch 6/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.4737 - acc: 0.8926 - val_loss: 0.8680 - val_acc: 0.7939\n",
    "Epoch 7/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.3505 - acc: 0.9240 - val_loss: 0.8449 - val_acc: 0.7974\n",
    "Epoch 8/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.2576 - acc: 0.9446 - val_loss: 0.8421 - val_acc: 0.7961\n",
    "Epoch 9/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1926 - acc: 0.9540 - val_loss: 0.8446 - val_acc: 0.8001\n",
    "Epoch 10/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1499 - acc: 0.9555 - val_loss: 0.8555 - val_acc: 0.7974\n",
    "Epoch 11/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1237 - acc: 0.9549 - val_loss: 0.8676 - val_acc: 0.7979\n",
    "Epoch 12/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1081 - acc: 0.9537 - val_loss: 0.8920 - val_acc: 0.7943\n",
    "Epoch 13/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0976 - acc: 0.9542 - val_loss: 0.9020 - val_acc: 0.7970\n",
    "Epoch 14/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0908 - acc: 0.9561 - val_loss: 0.9260 - val_acc: 0.7939\n",
    "Epoch 15/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0871 - acc: 0.9542 - val_loss: 0.9303 - val_acc: 0.7934\n",
    "Epoch 16/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0835 - acc: 0.9559 - val_loss: 0.9565 - val_acc: 0.7890\n",
    "Epoch 17/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0830 - acc: 0.9540 - val_loss: 0.9502 - val_acc: 0.8001\n",
    "Epoch 18/20\n",
    "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0791 - acc: 0.9556 - val_loss: 0.9794 - val_acc: 0.7952\n",
    "Epoch 19/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0775 - acc: 0.9550 - val_loss: 0.9802 - val_acc: 0.7996\n",
    "Epoch 20/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0772 - acc: 0.9539 - val_loss: 0.9943 - val_acc: 0.7930\n",
    "<keras.callbacks.History at 0x7f1d180509b0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=736; epochs=10\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  736 input_length:  400 batch_size:  32 epochs:  10\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_9 (Embedding)      (None, 400, 736)          68737984  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_5 (Glob (None, 736)               0         \n",
    "_________________________________________________________________\n",
    "dense_7 (Dense)              (None, 46)                33902     \n",
    "=================================================================\n",
    "Total params: 68,771,886\n",
    "Trainable params: 68,771,886\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/10\n",
    "8982/8982 [==============================] - 27s 3ms/step - loss: 2.0450 - acc: 0.5249 - val_loss: 1.5911 - val_acc: 0.6483\n",
    "Epoch 2/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 1.2851 - acc: 0.7197 - val_loss: 1.2060 - val_acc: 0.7275\n",
    "Epoch 3/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.8895 - acc: 0.8045 - val_loss: 0.9875 - val_acc: 0.7752\n",
    "Epoch 4/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.6182 - acc: 0.8567 - val_loss: 0.8814 - val_acc: 0.7943\n",
    "Epoch 5/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.4265 - acc: 0.9056 - val_loss: 0.8412 - val_acc: 0.7996\n",
    "Epoch 6/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.2886 - acc: 0.9370 - val_loss: 0.8272 - val_acc: 0.8085\n",
    "Epoch 7/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1989 - acc: 0.9505 - val_loss: 0.8363 - val_acc: 0.8059\n",
    "Epoch 8/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1475 - acc: 0.9544 - val_loss: 0.8427 - val_acc: 0.8063\n",
    "Epoch 9/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1213 - acc: 0.9539 - val_loss: 0.8640 - val_acc: 0.8072\n",
    "Epoch 10/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1068 - acc: 0.9550 - val_loss: 0.8905 - val_acc: 0.8041\n",
    "<keras.callbacks.History at 0x7f1d17b609e8>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T07:43:27.022931Z",
     "start_time": "2018-09-16T07:43:25.952197Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_i=\"./imdb_fasttext.pickle\"\n",
    "pickle_file_r=\"./reuters.pickle\"\n",
    "\n",
    "with open(pickle_file_i, 'rb') as f:\n",
    "    x_train_i = pickle.load(f)\n",
    "    y_train_i = pickle.load(f)\n",
    "    x_test_i = pickle.load(f)\n",
    "    y_test_i = pickle.load(f)\n",
    "with open(pickle_file_r, 'rb') as f:\n",
    "    x_train_r = pickle.load(f)\n",
    "    y_train_r = pickle.load(f)\n",
    "    x_test_r = pickle.load(f)\n",
    "    y_test_r = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T07:52:03.672687Z",
     "start_time": "2018-09-16T07:52:03.621149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibmdb   lenght(train and test):  25000 25000\n",
      "reuters lenght(train and test):  8982 2246 \n",
      "\n",
      "ibmdb     ave lenght(train and test):  238 230\n",
      "reuters   ave lenght(train and test):  145 147\n",
      "ibmdb     max lenght(train and test):  2494 2315\n",
      "reuters   max lenght(train and test):  2376 1032\n",
      "ibmdb     min lenght(train and test):  11 7\n",
      "reuters   min lenght(train and test):  13 2\n"
     ]
    }
   ],
   "source": [
    "print(\"ibmdb   lenght(train and test): \",len(x_train_i), len(x_test_i))\n",
    "print(\"reuters lenght(train and test): \",len(x_train_r), len(x_test_r),\"\\n\")\n",
    "\n",
    "print(\"ibmdb     ave lenght(train and test): \",\n",
    "    np.mean(list(map(len, x_train_i)), dtype=int),\n",
    "    np.mean(list(map(len, x_test_i)), dtype=int))\n",
    "print(\"reuters   ave lenght(train and test): \",\n",
    "    np.mean(list(map(len, x_train_r)), dtype=int),\n",
    "    np.mean(list(map(len, x_test_r)), dtype=int))\n",
    "\n",
    "print(\"ibmdb     max lenght(train and test): \",\n",
    "    np.max(list(map(len, x_train_i))),\n",
    "    np.max(list(map(len, x_test_i))))\n",
    "print(\"reuters   max lenght(train and test): \",\n",
    "    np.max(list(map(len, x_train_r))),\n",
    "    np.max(list(map(len, x_test_r))))\n",
    "\n",
    "print(\"ibmdb     min lenght(train and test): \",\n",
    "    np.min(list(map(len, x_train_i))),\n",
    "    np.min(list(map(len, x_test_i))))\n",
    "print(\"reuters   min lenght(train and test): \",\n",
    "    np.min(list(map(len, x_train_r))),\n",
    "    np.min(list(map(len, x_test_r))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロイターデータについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T08:13:44.215478Z",
     "start_time": "2018-09-16T08:13:44.209239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x_test_r:  2246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2245"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=[1,2,3,4,5,6,7]\n",
    "lst=list(map(len,x_test_r))\n",
    "print(\"len of x_test_r: \",len(lst))\n",
    "def bigger(_l):\n",
    "    return _l > 7\n",
    "    \n",
    "np.sum(list(map(bigger,lst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reuters_mlpデータをロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:20:56.864742Z",
     "start_time": "2018-09-16T14:20:56.856199Z"
    }
   },
   "outputs": [],
   "source": [
    "'''This example demonstrates the use of fasttext for text classification\n",
    "Based on Joulin et al's paper:\n",
    "Bags of Tricks for Efficient Text Classification\n",
    "https://arxiv.org/abs/1607.01759\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
    "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "# max_features = 20000\n",
    "max_features = 2500\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:17:11.205410Z",
     "start_time": "2018-09-16T14:17:10.219778Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "max_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:17:11.613447Z",
     "start_time": "2018-09-16T14:17:11.537652Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_file_r = \"./reuters.pickle\"\n",
    "with open(pickle_file_r, 'wb') as f:\n",
    "  pickle.dump(x_train, f)\n",
    "  pickle.dump(y_train, f)\n",
    "  pickle.dump(x_test, f)\n",
    "  pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:05.187843Z",
     "start_time": "2018-09-16T14:21:05.076500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file=\"./reuters.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "    y_train = pickle.load(f)\n",
    "    x_test = pickle.load(f)\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:09.297068Z",
     "start_time": "2018-09-16T14:21:09.287726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 train sequences\n",
      "2246 test sequences\n",
      "Average train sequence length: 145\n",
      "Average test sequence length: 147\n"
     ]
    }
   ],
   "source": [
    "l_x_train = len(x_train)\n",
    "l_x_test = len(x_test)\n",
    "m_x_train =  np.mean(list(map(len, x_train)), dtype=int)\n",
    "m_x_test =  np.mean(list(map(len, x_test)), dtype=int)\n",
    "\n",
    "print(l_x_train, 'train sequences')\n",
    "print(l_x_test, 'test sequences')\n",
    "print('Average train sequence length: {}'.format(m_x_train))\n",
    "print('Average test sequence length: {}'.format(m_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:15.790275Z",
     "start_time": "2018-09-16T14:21:13.833476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "max_features before np.max() 2500\n",
      "max_features after  np.max() 93394\n",
      "Average train sequence length: 290\n",
      "Average test sequence length: 289\n",
      "CPU times: user 1.94 s, sys: 5.85 ms, total: 1.95 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    print(\"max_features before np.max()\",max_features)\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "    print(\"max_features after  np.max()\",max_features)\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:25.995722Z",
     "start_time": "2018-09-16T14:21:25.991828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "befoe ngram\n",
      "Average train sequence length: 145\n",
      "Average test sequence length: 147\n",
      "max(y_train):  45\n"
     ]
    }
   ],
   "source": [
    "# l_x_train = len(x_train)\n",
    "# l_x_test = len(x_test)\n",
    "# m_x_train =  np.mean(list(map(len, x_train)), dtype=int)\n",
    "# m_x_test =  np.mean(list(map(len, x_test)), dtype=int)\n",
    "\n",
    "# print(l_x_train, 'train sequences')\n",
    "# print(l_x_test, 'test sequences')\n",
    "print(\"\\nbefoe ngram\")\n",
    "print('Average train sequence length: {}'.format(m_x_train))\n",
    "print('Average test sequence length: {}'.format(m_x_test))\n",
    "print(\"max(y_train): \", max(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:34.417392Z",
     "start_time": "2018-09-16T14:21:34.090661Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (8982, 400)\n",
      "x_test shape: (2246, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:37.858926Z",
     "start_time": "2018-09-16T14:21:37.855372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features:  93394\n",
      "embedding_dims:  50\n",
      "maxlen:  400\n",
      "batch_size:  32\n",
      "epochs:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"max_features: \",max_features)\n",
    "print(\"embedding_dims: \",embedding_dims)\n",
    "print(\"maxlen: \",maxlen)\n",
    "print(\"batch_size: \",batch_size)\n",
    "print(\"epochs: \",epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:21:43.370264Z",
     "start_time": "2018-09-16T14:21:43.367141Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:30:45.210266Z",
     "start_time": "2018-09-16T14:21:51.345456Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features:  93394 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/5\n",
      "8982/8982 [==============================] - 108s 12ms/step - loss: 2.5311 - acc: 0.3899 - val_loss: 2.0695 - val_acc: 0.4417\n",
      "Epoch 2/5\n",
      "8982/8982 [==============================] - 104s 12ms/step - loss: 1.9066 - acc: 0.4923 - val_loss: 1.8054 - val_acc: 0.5654\n",
      "Epoch 3/5\n",
      "8982/8982 [==============================] - 107s 12ms/step - loss: 1.6344 - acc: 0.6181 - val_loss: 1.6014 - val_acc: 0.6345\n",
      "Epoch 4/5\n",
      "8982/8982 [==============================] - 107s 12ms/step - loss: 1.4095 - acc: 0.6777 - val_loss: 1.4489 - val_acc: 0.6719\n",
      "Epoch 5/5\n",
      "8982/8982 [==============================] - 108s 12ms/step - loss: 1.2326 - acc: 0.7144 - val_loss: 1.3383 - val_acc: 0.6901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fa6702470>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dims=184\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T13:13:02.969797Z",
     "start_time": "2018-09-16T13:13:02.966825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 (8982, 400) (8982,)\n",
      "2 1 (2246, 400) (2246,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.ndim,y_train.ndim,x_train.shape,y_train.shape)\n",
    "print(x_test.ndim,y_test.ndim,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初Keras\n",
    "## ログ\n",
    "- まず、kerasの[examples][1]を見てみる。\n",
    "- NLPに関心があるので、「Text & sequences examples」のパートを見てみる。\n",
    "- reutersに惹かれて[reuters_mlp.py][2]をそのままで実行してみる。  \n",
    "    Test accuracy: 0.792520035618878\n",
    "- binaryオプションを使用しているのでtfidfにしてみるがほぼ同じ結果。\n",
    "- ロイター以外のサンプルを見てみる。幾つかのものに、accuracyが記載されている。\n",
    "- accuracyが記載されているものから[imdb_fasttext.py][3]を見てみる。  \n",
    "  fasttextは元のデータにngramのペアに割り振ったID(元のデータにあるngramペアのID)を追加するというもの。速くて精度が良いとのこと。  \n",
    "- ロイターのデータにfasttextを適用してみようと思う。\n",
    "- 寄り道\n",
    "    - [CS231n: Convolutional Neural Networks for Visual Recognition. ][4]\n",
    "    - 特に、[CS231n Convolutional Neural Networks for Visual Recognition][5]  \n",
    "    - [Multi class classification with LSTM by Peter Nagy][6]  \n",
    "        SpatialDropout1D、LSTM、optimizer='adam'などが使われている。\n",
    "    - [Understanding LSTM Networks][7]  \n",
    "    - [A Step by Step Backpropagation Example][8]  \n",
    "    - [https://www.kaggle.com/carlosaguayo/deep-learning-for-text-classification][9] \n",
    "    - [How to Develop a Word-Level Neural Language Model and Use it to Generate Text][10]  \n",
    "\n",
    "[1]: https://github.com/keras-team/keras/tree/master/examples\n",
    "[2]: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "[3]: https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py\n",
    "[5]: http://cs231n.github.io/convolutional-networks/\n",
    "[4]: http://cs231n.stanford.edu/\n",
    "[6]: https://www.kaggle.com/ngyptr/multi-class-classification-with-lstm\n",
    "[7]: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "[8]: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "[9]: https://www.kaggle.com/carlosaguayo/deep-learning-for-text-classification\n",
    "[10]: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "## 目標と結果\n",
    "reutersのデータはimdbのデータに比べると数も少なく、平均の長さも短い。そのため、オリジナルのreuters_mlp.pyの結果をベンチマークにする。具体的にはテストデータでのaccuracy=0.8を目指す。  \n",
    "結果、Test accuracy: 0.8036509349424714となる(4.14)。\n",
    "\n",
    "## 変更点\n",
    "- embedding_dimsを大きく増やす\n",
    "- max_featuresは精度に若干影響\n",
    "- GlobalAveragePooling1D()をGlobalMaxPooling1D()に変更\n",
    "- activationをsoftmaxに\n",
    "- loss=categorical_crossentropyに  \n",
    "\n",
    "## その他\n",
    "- 途中まではPCで実施。途中からGoogle Colaboratryを使用\n",
    "- 上手なログの残し方がわからない。。。\n",
    "- epochsを増やしてみたが５位で良さそう\n",
    "- ロイターのデータには長さが２のものがあるが極端に短いデータとしてはこれのみのためそのままにする\n",
    "- embedding_dimsを1472まで増やすと以下のUserWarningがでる。しかし、WarningでかつColab環境で動いているので対処せず。。。\n",
    "    ```\n",
    "    UserWarning: Converting sparse IndexedSlices to a dense Tensor with 137475968 elements. This may consume a large amount of memory.\n",
    "    ```\n",
    "- fasttextにreutersデータをロードする形態で処理していたが、imdb_fasttextとreuters_mlpとでvalidation/testの取扱が若干違うので最後にここを修正。reuters_mlpの形態で処理を行うように(「reuters版モデルでのevaluate()」)\n",
    "- せっかくなので次はConv1Dを使ってみようと思う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:16:13.329142Z",
     "start_time": "2018-09-17T09:16:13.319197Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:16:13.859492Z",
     "start_time": "2018-09-17T09:16:13.843271Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle_file=\"./imdb_fasttext.pickle\"\n",
    "\n",
    "# with open(pickle_file, 'wb') as f:\n",
    "#     pickle.dump(x_train, f)\n",
    "#     pickle.dump(y_train, f)\n",
    "#     pickle.dump(x_test, f)\n",
    "#     pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:16:19.922745Z",
     "start_time": "2018-09-17T09:16:18.458387Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_i=\"./imdb_fasttext.pickle\"\n",
    "pickle_file_r=\"./reuters.pickle\"\n",
    "\n",
    "with open(pickle_file_i, 'rb') as f:\n",
    "    x_train_i = pickle.load(f)\n",
    "    y_train_i = pickle.load(f)\n",
    "    x_test_i = pickle.load(f)\n",
    "    y_test_i = pickle.load(f)\n",
    "with open(pickle_file_r, 'rb') as f:\n",
    "    x_train_r = pickle.load(f)\n",
    "    y_train_r = pickle.load(f)\n",
    "    x_test_r = pickle.load(f)\n",
    "    y_test_r = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:16:19.993638Z",
     "start_time": "2018-09-17T09:16:19.926780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibmdb   length(train and test):  25000 25000\n",
      "reuters length(train and test):  8982 2246 \n",
      "\n",
      "ibmdb     ave length(train and test):  238 230\n",
      "reuters   ave length(train and test):  145 147\n",
      "ibmdb     max length(train and test):  2494 2315\n",
      "reuters   max length(train and test):  2376 1032\n",
      "ibmdb     min length(train and test):  11 7\n",
      "reuters   min length(train and test):  13 2\n"
     ]
    }
   ],
   "source": [
    "print(\"ibmdb   length(train and test): \",len(x_train_i), len(x_test_i))\n",
    "print(\"reuters length(train and test): \",len(x_train_r), len(x_test_r),\"\\n\")\n",
    "\n",
    "print(\"ibmdb     ave length(train and test): \",\n",
    "    np.mean(list(map(len, x_train_i)), dtype=int),\n",
    "    np.mean(list(map(len, x_test_i)), dtype=int))\n",
    "print(\"reuters   ave length(train and test): \",\n",
    "    np.mean(list(map(len, x_train_r)), dtype=int),\n",
    "    np.mean(list(map(len, x_test_r)), dtype=int))\n",
    "\n",
    "print(\"ibmdb     max length(train and test): \",\n",
    "    np.max(list(map(len, x_train_i))),\n",
    "    np.max(list(map(len, x_test_i))))\n",
    "print(\"reuters   max length(train and test): \",\n",
    "    np.max(list(map(len, x_train_r))),\n",
    "    np.max(list(map(len, x_test_r))))\n",
    "\n",
    "print(\"ibmdb     min length(train and test): \",\n",
    "    np.min(list(map(len, x_train_i))),\n",
    "    np.min(list(map(len, x_test_i))))\n",
    "print(\"reuters   min length(train and test): \",\n",
    "    np.min(list(map(len, x_train_r))),\n",
    "    np.min(list(map(len, x_test_r))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロイターデータについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:16:24.220550Z",
     "start_time": "2018-09-17T09:16:24.180256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x_test_r:  2246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2245"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=list(map(len,x_test_r))\n",
    "print(\"len of x_test_r: \",len(lst))\n",
    "def bigger(_l):\n",
    "    return _l > 7 # 7=imdbのmin length\n",
    "    \n",
    "np.sum(list(map(bigger,lst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttextでreutersデータを処理\n",
    "以下のセルはimdb_fasttext.pyの出だしそのまま。ngram_rangeとmax_featuresを変更している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:16:26.458972Z",
     "start_time": "2018-09-17T09:16:26.413765Z"
    }
   },
   "outputs": [],
   "source": [
    "'''This example demonstrates the use of fasttext for text classification\n",
    "Based on Joulin et al's paper:\n",
    "Bags of Tricks for Efficient Text Classification\n",
    "https://arxiv.org/abs/1607.01759\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
    "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "# max_features = 20000\n",
    "max_features = 2500\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:41.177037Z",
     "start_time": "2018-09-17T09:22:40.612656Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "max_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:44.942242Z",
     "start_time": "2018-09-17T09:22:44.938850Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle_file_r = \"./reuters.pickle\"\n",
    "# with open(pickle_file_r, 'wb') as f:\n",
    "#   pickle.dump(x_train, f)\n",
    "#   pickle.dump(y_train, f)\n",
    "#   pickle.dump(x_test, f)\n",
    "#   pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:45.405079Z",
     "start_time": "2018-09-17T09:22:45.394107Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle_file=\"./reuters.pickle\"\n",
    "\n",
    "# with open(pickle_file, 'rb') as f:\n",
    "#     x_train = pickle.load(f)\n",
    "#     y_train = pickle.load(f)\n",
    "#     x_test = pickle.load(f)\n",
    "#     y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:46.350905Z",
     "start_time": "2018-09-17T09:22:46.321000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 train sequences\n",
      "2246 test sequences\n",
      "Average train sequence length: 145\n",
      "Average test sequence length: 147\n"
     ]
    }
   ],
   "source": [
    "l_x_train = len(x_train)\n",
    "l_x_test = len(x_test)\n",
    "m_x_train =  np.mean(list(map(len, x_train)), dtype=int)\n",
    "m_x_test =  np.mean(list(map(len, x_test)), dtype=int)\n",
    "\n",
    "print(l_x_train, 'train sequences')\n",
    "print(l_x_test, 'test sequences')\n",
    "print('Average train sequence length: {}'.format(m_x_train))\n",
    "print('Average test sequence length: {}'.format(m_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:49.718199Z",
     "start_time": "2018-09-17T09:22:47.766013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "max_features before np.max() 110894\n",
      "max_features after  np.max() 201788\n",
      "Average train sequence length: 290\n",
      "Average test sequence length: 289\n",
      "CPU times: user 1.93 s, sys: 7 ms, total: 1.93 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    print(\"max_features before np.max()\",max_features)\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "    print(\"max_features after  np.max()\",max_features)\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:50.102374Z",
     "start_time": "2018-09-17T09:22:49.720080Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (8982, 400)\n",
      "x_test shape: (2246, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T09:22:51.177071Z",
     "start_time": "2018-09-17T09:22:51.167789Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オリジナルモデルからのfit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-17T09:22:54.314Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=1472; epochs=5\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retuter版モデルでのevaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-16T14:30:45.210266Z",
     "start_time": "2018-09-16T14:21:51.345456Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=1472; epochs=5\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "#           validation_data=(x_test, y_test))\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果\n",
    "## imdb_fasttext.py(オリジナル。ngram=1)\n",
    "acc:92.5% & val_acc:88.8%  total: 4min 49s\n",
    "## imdb_fasttext.py(オリジナル。ngram=2)\n",
    "acc:99.5% & val_acc:90.4%  total: 3h 4s\n",
    "## imdb_fasttext.py(colab。ngram=2)\n",
    "acc:99.5% & val_acc:90.4%  total:5m 5s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  110894 embedding_dims:  50 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 4s 401us/step - loss: 2.8477 - acc: 0.3599 - val_loss: 2.2456 - val_acc: 0.3980\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 3s 347us/step - loss: 2.1460 - acc: 0.3957 - val_loss: 2.0688 - val_acc: 0.4239\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 3s 361us/step - loss: 1.9780 - acc: 0.4505 - val_loss: 1.9322 - val_acc: 0.4835\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 3s 361us/step - loss: 1.8313 - acc: 0.5137 - val_loss: 1.8073 - val_acc: 0.5508\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 3s 356us/step - loss: 1.6894 - acc: 0.5916 - val_loss: 1.6923 - val_acc: 0.6046\n",
    "<keras.callbacks.History at 0x7f2b21207320>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=92\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  110894 embedding_dims:  92 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 5s 560us/step - loss: 2.6932 - acc: 0.3784 - val_loss: 2.1609 - val_acc: 0.4127\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 5s 507us/step - loss: 2.0320 - acc: 0.4378 - val_loss: 1.9462 - val_acc: 0.4728\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 5s 517us/step - loss: 1.8201 - acc: 0.5331 - val_loss: 1.7741 - val_acc: 0.5761\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 5s 526us/step - loss: 1.6302 - acc: 0.6240 - val_loss: 1.6287 - val_acc: 0.6327\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 5s 528us/step - loss: 1.4653 - acc: 0.6716 - val_loss: 1.5118 - val_acc: 0.6621\n",
    "<keras.callbacks.History at 0x7f2b232df1d0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=184\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  110894 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 9s 970us/step - loss: 2.5318 - acc: 0.3890 - val_loss: 2.0511 - val_acc: 0.4479\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 8s 865us/step - loss: 1.8905 - acc: 0.5070 - val_loss: 1.7928 - val_acc: 0.5712\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 8s 867us/step - loss: 1.6199 - acc: 0.6264 - val_loss: 1.5926 - val_acc: 0.6438\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 8s 866us/step - loss: 1.4012 - acc: 0.6818 - val_loss: 1.4466 - val_acc: 0.6687\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 8s 872us/step - loss: 1.2287 - acc: 0.7147 - val_loss: 1.3363 - val_acc: 0.6910\n",
    "<keras.callbacks.History at 0x7f2b22c4fcf8>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=184\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 2.5311 - acc: 0.3899 - val_loss: 2.0695 - val_acc: 0.4417\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 104s 12ms/step - loss: 1.9066 - acc: 0.4923 - val_loss: 1.8054 - val_acc: 0.5654\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.6344 - acc: 0.6181 - val_loss: 1.6014 - val_acc: 0.6345\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.4095 - acc: 0.6777 - val_loss: 1.4489 - val_acc: 0.6719\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 1.2326 - acc: 0.7144 - val_loss: 1.3383 - val_acc: 0.6901\n",
    "<keras.callbacks.History at 0x7f5fa6702470>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "max_features:  93394 embedding_dims:  184 input_length:  400 batch_size:  32 epochs:  5\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 2.5311 - acc: 0.3899 - val_loss: 2.0695 - val_acc: 0.4417\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 104s 12ms/step - loss: 1.9066 - acc: 0.4923 - val_loss: 1.8054 - val_acc: 0.5654\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.6344 - acc: 0.6181 - val_loss: 1.6014 - val_acc: 0.6345\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 107s 12ms/step - loss: 1.4095 - acc: 0.6777 - val_loss: 1.4489 - val_acc: 0.6719\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 108s 12ms/step - loss: 1.2326 - acc: 0.7144 - val_loss: 1.3383 - val_acc: 0.6901\n",
    "<keras.callbacks.History at 0x7f5fa6702470>\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  5\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_2 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_average_pooling1d_2 ( (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.3918 - acc: 0.4105 - val_loss: 1.9521 - val_acc: 0.4884\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.7406 - acc: 0.5765 - val_loss: 1.6333 - val_acc: 0.6269\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4137 - acc: 0.6798 - val_loss: 1.4235 - val_acc: 0.6750\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.1830 - acc: 0.7267 - val_loss: 1.2888 - val_acc: 0.7008\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.0108 - acc: 0.7735 - val_loss: 1.1930 - val_acc: 0.7302\n",
    "<keras.callbacks.History at 0x7f1d5259ec50>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=368\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  5\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_5 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/5\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.2130 - acc: 0.4660 - val_loss: 1.7213 - val_acc: 0.5726\n",
    "Epoch 2/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4644 - acc: 0.6672 - val_loss: 1.3551 - val_acc: 0.6848\n",
    "Epoch 3/5\n",
    "8982/8982 [==============================] - 13s 1ms/step - loss: 1.0976 - acc: 0.7552 - val_loss: 1.1304 - val_acc: 0.7453\n",
    "Epoch 4/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.8324 - acc: 0.8201 - val_loss: 0.9905 - val_acc: 0.7725\n",
    "Epoch 5/5\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.6292 - acc: 0.8595 - val_loss: 0.9165 - val_acc: 0.7845\n",
    "<keras.callbacks.History at 0x7f1d18481908>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=368; epochs=10\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  10\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_7 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_3 (Glob (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/10\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.2225 - acc: 0.4787 - val_loss: 1.7340 - val_acc: 0.5557\n",
    "Epoch 2/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4701 - acc: 0.6730 - val_loss: 1.3616 - val_acc: 0.6906\n",
    "Epoch 3/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.0971 - acc: 0.7561 - val_loss: 1.1406 - val_acc: 0.7413\n",
    "Epoch 4/10\n",
    "8982/8982 [==============================] - 13s 1ms/step - loss: 0.8271 - acc: 0.8169 - val_loss: 0.9997 - val_acc: 0.7783\n",
    "Epoch 5/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.6232 - acc: 0.8559 - val_loss: 0.9194 - val_acc: 0.7832\n",
    "Epoch 6/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.4669 - acc: 0.8967 - val_loss: 0.8753 - val_acc: 0.7925\n",
    "Epoch 7/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.3441 - acc: 0.9255 - val_loss: 0.8531 - val_acc: 0.7956\n",
    "Epoch 8/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.2522 - acc: 0.9432 - val_loss: 0.8475 - val_acc: 0.7979\n",
    "Epoch 9/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1897 - acc: 0.9532 - val_loss: 0.8557 - val_acc: 0.8001\n",
    "Epoch 10/10\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1489 - acc: 0.9548 - val_loss: 0.8640 - val_acc: 0.7983\n",
    "<keras.callbacks.History at 0x7f1d181039b0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=368; epochs=20\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  368 input_length:  400 batch_size:  32 epochs:  20\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_8 (Embedding)      (None, 400, 368)          34368992  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_4 (Glob (None, 368)               0         \n",
    "_________________________________________________________________\n",
    "dense_6 (Dense)              (None, 46)                16974     \n",
    "=================================================================\n",
    "Total params: 34,385,966\n",
    "Trainable params: 34,385,966\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/20\n",
    "8982/8982 [==============================] - 14s 2ms/step - loss: 2.2099 - acc: 0.4986 - val_loss: 1.7325 - val_acc: 0.5508\n",
    "Epoch 2/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.4735 - acc: 0.6650 - val_loss: 1.3595 - val_acc: 0.6923\n",
    "Epoch 3/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 1.1021 - acc: 0.7585 - val_loss: 1.1299 - val_acc: 0.7467\n",
    "Epoch 4/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.8327 - acc: 0.8174 - val_loss: 0.9942 - val_acc: 0.7765\n",
    "Epoch 5/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.6300 - acc: 0.8577 - val_loss: 0.9140 - val_acc: 0.7827\n",
    "Epoch 6/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.4737 - acc: 0.8926 - val_loss: 0.8680 - val_acc: 0.7939\n",
    "Epoch 7/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.3505 - acc: 0.9240 - val_loss: 0.8449 - val_acc: 0.7974\n",
    "Epoch 8/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.2576 - acc: 0.9446 - val_loss: 0.8421 - val_acc: 0.7961\n",
    "Epoch 9/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1926 - acc: 0.9540 - val_loss: 0.8446 - val_acc: 0.8001\n",
    "Epoch 10/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1499 - acc: 0.9555 - val_loss: 0.8555 - val_acc: 0.7974\n",
    "Epoch 11/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1237 - acc: 0.9549 - val_loss: 0.8676 - val_acc: 0.7979\n",
    "Epoch 12/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.1081 - acc: 0.9537 - val_loss: 0.8920 - val_acc: 0.7943\n",
    "Epoch 13/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0976 - acc: 0.9542 - val_loss: 0.9020 - val_acc: 0.7970\n",
    "Epoch 14/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0908 - acc: 0.9561 - val_loss: 0.9260 - val_acc: 0.7939\n",
    "Epoch 15/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0871 - acc: 0.9542 - val_loss: 0.9303 - val_acc: 0.7934\n",
    "Epoch 16/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0835 - acc: 0.9559 - val_loss: 0.9565 - val_acc: 0.7890\n",
    "Epoch 17/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0830 - acc: 0.9540 - val_loss: 0.9502 - val_acc: 0.8001\n",
    "Epoch 18/20\n",
    "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0791 - acc: 0.9556 - val_loss: 0.9794 - val_acc: 0.7952\n",
    "Epoch 19/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0775 - acc: 0.9550 - val_loss: 0.9802 - val_acc: 0.7996\n",
    "Epoch 20/20\n",
    "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0772 - acc: 0.9539 - val_loss: 0.9943 - val_acc: 0.7930\n",
    "<keras.callbacks.History at 0x7f1d180509b0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "embedding_dims=736; epochs=10\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  736 input_length:  400 batch_size:  32 epochs:  10\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_9 (Embedding)      (None, 400, 736)          68737984  \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_5 (Glob (None, 736)               0         \n",
    "_________________________________________________________________\n",
    "dense_7 (Dense)              (None, 46)                33902     \n",
    "=================================================================\n",
    "Total params: 68,771,886\n",
    "Trainable params: 68,771,886\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/10\n",
    "8982/8982 [==============================] - 27s 3ms/step - loss: 2.0450 - acc: 0.5249 - val_loss: 1.5911 - val_acc: 0.6483\n",
    "Epoch 2/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 1.2851 - acc: 0.7197 - val_loss: 1.2060 - val_acc: 0.7275\n",
    "Epoch 3/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.8895 - acc: 0.8045 - val_loss: 0.9875 - val_acc: 0.7752\n",
    "Epoch 4/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.6182 - acc: 0.8567 - val_loss: 0.8814 - val_acc: 0.7943\n",
    "Epoch 5/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.4265 - acc: 0.9056 - val_loss: 0.8412 - val_acc: 0.7996\n",
    "Epoch 6/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.2886 - acc: 0.9370 - val_loss: 0.8272 - val_acc: 0.8085\n",
    "Epoch 7/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1989 - acc: 0.9505 - val_loss: 0.8363 - val_acc: 0.8059\n",
    "Epoch 8/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1475 - acc: 0.9544 - val_loss: 0.8427 - val_acc: 0.8063\n",
    "Epoch 9/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1213 - acc: 0.9539 - val_loss: 0.8640 - val_acc: 0.8072\n",
    "Epoch 10/10\n",
    "8982/8982 [==============================] - 24s 3ms/step - loss: 0.1068 - acc: 0.9550 - val_loss: 0.8905 - val_acc: 0.8041\n",
    "<keras.callbacks.History at 0x7f1d17b609e8>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=1472; epochs=10\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "max_features:  93394 embedding_dims:  1472 input_length:  400 batch_size:  32 epochs:  10\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_2 (Embedding)      (None, 400, 1472)         137475968 \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_2 (Glob (None, 1472)              0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 46)                67758     \n",
    "=================================================================\n",
    "Total params: 137,543,726\n",
    "Trainable params: 137,543,726\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 137475968 elements. This may consume a large amount of memory.\n",
    "  num_elements)\n",
    "Train on 8982 samples, validate on 2246 samples\n",
    "Epoch 1/10\n",
    "8982/8982 [==============================] - 51s 6ms/step - loss: 1.8967 - acc: 0.5583 - val_loss: 1.4338 - val_acc: 0.6790\n",
    "Epoch 2/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 1.1007 - acc: 0.7586 - val_loss: 1.0430 - val_acc: 0.7685\n",
    "Epoch 3/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.7005 - acc: 0.8413 - val_loss: 0.8760 - val_acc: 0.7939\n",
    "Epoch 4/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.4422 - acc: 0.9005 - val_loss: 0.8209 - val_acc: 0.8010\n",
    "Epoch 5/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.2747 - acc: 0.9380 - val_loss: 0.8102 - val_acc: 0.8099\n",
    "Epoch 6/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.1797 - acc: 0.9527 - val_loss: 0.8176 - val_acc: 0.8103\n",
    "Epoch 7/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.1394 - acc: 0.9534 - val_loss: 0.8490 - val_acc: 0.8090\n",
    "Epoch 8/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.1202 - acc: 0.9520 - val_loss: 0.8706 - val_acc: 0.8041\n",
    "Epoch 9/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.1103 - acc: 0.9546 - val_loss: 0.9036 - val_acc: 0.8028\n",
    "Epoch 10/10\n",
    "8982/8982 [==============================] - 46s 5ms/step - loss: 0.1055 - acc: 0.9537 - val_loss: 0.9195 - val_acc: 0.8032\n",
    "<keras.callbacks.History at 0x7fb17724f358>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=1472; epochs=5\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "#           validation_data=(x_test, y_test))\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "max_features:  93394 embedding_dims:  1472 input_length:  400 batch_size:  32 epochs:  5\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 400, 1472)         137475968 \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 1472)              0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 46)                67758     \n",
    "=================================================================\n",
    "Total params: 137,543,726\n",
    "Trainable params: 137,543,726\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 137475968 elements. This may consume a large amount of memory.\n",
    "  num_elements)\n",
    "Train on 8083 samples, validate on 899 samples\n",
    "Epoch 1/5\n",
    "8083/8083 [==============================] - 47s 6ms/step - loss: 1.9442 - acc: 0.5418 - val_loss: 1.5325 - val_acc: 0.6852\n",
    "Epoch 2/5\n",
    "8083/8083 [==============================] - 42s 5ms/step - loss: 1.1606 - acc: 0.7449 - val_loss: 1.1730 - val_acc: 0.7608\n",
    "Epoch 3/5\n",
    "8083/8083 [==============================] - 42s 5ms/step - loss: 0.7462 - acc: 0.8331 - val_loss: 1.0088 - val_acc: 0.7786\n",
    "Epoch 4/5\n",
    "8083/8083 [==============================] - 42s 5ms/step - loss: 0.4752 - acc: 0.8958 - val_loss: 0.9414 - val_acc: 0.7909\n",
    "Epoch 5/5\n",
    "8083/8083 [==============================] - 42s 5ms/step - loss: 0.2932 - acc: 0.9333 - val_loss: 0.9020 - val_acc: 0.7987\n",
    "2246/2246 [==============================] - 0s 131us/step\n",
    "Test score: 0.8289790170487823\n",
    "Test accuracy: 0.8036509349424714\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttextロジックでロイターデータ処理\n",
    "```\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "embedding_dims=1472; epochs=5\n",
    "print(\"max_features: \",max_features,\"embedding_dims: \",embedding_dims,\"input_length: \",maxlen,\"batch_size: \",batch_size,\"epochs: \",epochs)\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "#           validation_data=(x_test, y_test))\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "max_features:  110894 embedding_dims:  1472 input_length:  400 batch_size:  32 epochs:  5\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 400, 1472)         163235968 \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_1 (Glob (None, 1472)              0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 46)                67758     \n",
    "=================================================================\n",
    "Total params: 163,303,726\n",
    "Trainable params: 163,303,726\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 163235968 elements. This may consume a large amount of memory.\n",
    "  num_elements)\n",
    "Train on 8083 samples, validate on 899 samples\n",
    "Epoch 1/5\n",
    "8083/8083 [==============================] - 55s 7ms/step - loss: 1.9487 - acc: 0.5373 - val_loss: 1.5347 - val_acc: 0.6852\n",
    "Epoch 2/5\n",
    "8083/8083 [==============================] - 49s 6ms/step - loss: 1.1604 - acc: 0.7461 - val_loss: 1.1674 - val_acc: 0.7575\n",
    "Epoch 3/5\n",
    "8083/8083 [==============================] - 49s 6ms/step - loss: 0.7451 - acc: 0.8311 - val_loss: 1.0034 - val_acc: 0.7731\n",
    "Epoch 4/5\n",
    "8083/8083 [==============================] - 49s 6ms/step - loss: 0.4765 - acc: 0.8936 - val_loss: 0.9382 - val_acc: 0.7842\n",
    "Epoch 5/5\n",
    "8083/8083 [==============================] - 49s 6ms/step - loss: 0.2951 - acc: 0.9328 - val_loss: 0.8963 - val_acc: 0.7931\n",
    "2246/2246 [==============================] - 0s 127us/step\n",
    "Test score: 0.8307678548756817\n",
    "Test accuracy: 0.7983081033478224\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
